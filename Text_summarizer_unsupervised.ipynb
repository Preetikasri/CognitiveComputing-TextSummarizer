{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "import gc\n",
    "import string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('amazon-fine-food-reviews/Reviews.csv', encoding='utf-8')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def split_sentences(emails):\n",
    "    \"\"\"\n",
    "    Splits the reviews into individual sentences\n",
    "    \"\"\"\n",
    "    n_emails = len(emails)\n",
    "    for i in range(n_emails):\n",
    "        email = emails[i]\n",
    "        #print(email)\n",
    "        sentences = sent_tokenize(email)\n",
    "        #print(sentences)\n",
    "        for j in reversed(range(len(sentences))):\n",
    "            sent = sentences[j]\n",
    "            sentences[j] = sent.strip()\n",
    "            if sent == '':\n",
    "                sentences.pop(j)\n",
    "        emails[i] = sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rev_list = list(df['Text'])\n",
    "split_sentences(rev_list)\n",
    "#Adding split reviews in the data frame\n",
    "df['sent_tokens'] = rev_list\n",
    "#Calculating lenght of sentences in each review\n",
    "df['length_of_rv'] = df['sent_tokens'].map(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(172765, 12)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choice_length = 5\n",
    "df = df[df['length_of_rv']>choice_length]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Making vocabulary with reviews with max vocabs=5000. \n",
    "list_sentences_train = df['Text']\n",
    "max_features = 5000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "maxlen = 200\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadEmbeddingMatrix(typeToLoad):\n",
    "        #load different embedding file from Kaggle depending on which embedding \n",
    "        #matrix we are going to experiment with\n",
    "        if(typeToLoad==\"glove\"):\n",
    "            EMBEDDING_FILE='glove.twitter.27B.25d.txt/glove.twitter.27B.25d.txt'\n",
    "            embed_size = 25\n",
    "        \n",
    "        elif(typeToLoad==\"fasttext\"):\n",
    "            EMBEDDING_FILE='wiki.simple.vec/wiki.simple.vec'\n",
    "            embed_size = 300\n",
    "\n",
    "        if(typeToLoad==\"glove\" or typeToLoad==\"fasttext\" ):\n",
    "            embeddings_index = dict()\n",
    "            #Transfer the embedding weights into a dictionary by iterating through every line of the file.\n",
    "            f = open(EMBEDDING_FILE, encoding='utf-8')\n",
    "            for line in f:\n",
    "                #split up line into an indexed array\n",
    "                values = line.split()\n",
    "                #first index is word\n",
    "                word = values[0]\n",
    "                #store the rest of the values in the array as a new array\n",
    "                coefs = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings_index[word] = coefs #50 dimensions\n",
    "            f.close()\n",
    "            print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "       # else:\n",
    "        #    embeddings_index = dict()\n",
    "         #   for word in word2vecDict.wv.vocab:\n",
    "          #      embeddings_index[word] = word2vecDict.word_vec(word)\n",
    "           # print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "            \n",
    "        gc.collect()\n",
    "        return embeddings_index #, embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1193515 word vectors.\n"
     ]
    }
   ],
   "source": [
    "## Loading 'glove' words\n",
    "emb_index= loadEmbeddingMatrix('glove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_sentence_embedding(wordList):\n",
    "    \"\"\"\n",
    "    This function calculates the embedding for entire sentence by taking the mean of embedding of\n",
    "    each word in the sentence. To be improved. \n",
    "    \"\"\"\n",
    "    #wordList = re.sub(\"[^\\w]\", \" \",  sent).split()\n",
    "    #print(wordList)\n",
    "    emb_li =[]\n",
    "    for k in wordList:\n",
    "        embedding_vector = emb_index.get(k)\n",
    "        if embedding_vector is not None:\n",
    "            if(len(embedding_vector) == 25):\n",
    "                emb_li.append(list(embedding_vector))\n",
    "    #print(\"Lost words in translation: \", len(wordList)-len(emb_li))\n",
    "    mean_arr = np.array(emb_li)\n",
    "    #print(\"done calculating sentence emb for you\")\n",
    "    return np.mean(mean_arr, axis=0)\n",
    "\n",
    "\n",
    "def get_sent_embedding(mylist):\n",
    "    \"\"\"\n",
    "    This function calculates the embedding of each sentence in the review. Checks if the sentence being passed is a valid one, \n",
    "    removing the punctuation and emojis etc.\n",
    "    \"\"\"\n",
    "    sent_emb = []\n",
    "    n_sentences = len(mylist)\n",
    "    for i in mylist:\n",
    "        #print(\"my sentence : \", i)\n",
    "        #print(\"\\nlower is\", i.lower())\n",
    "        i = i.lower()\n",
    "        wL = re.sub(\"[^\\w]\", \" \",  i).split()\n",
    "        if(len(wL)>0):\n",
    "            for k in wL:\n",
    "                if(k in string.punctuation):\n",
    "                    wL.remove(k)\n",
    "            if(len(wL) <= 2):\n",
    "                continue\n",
    "        else:\n",
    "            print(\"Sentence Removed: \",i)\n",
    "            continue\n",
    "        #print(wL)\n",
    "        res = list(calculate_sentence_embedding(wL))\n",
    "        sent_emb.append(res)\n",
    "    return np.array(sent_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "## Calculating summaries for first 5000 reviews. \n",
    "how_many_summaries = 5000\n",
    "summary = [None]*how_many_summaries\n",
    "for rv in range(how_many_summaries):\n",
    "        review = df['sent_tokens'].iloc[rv]\n",
    "        enc_email = get_sent_embedding(review)\n",
    "        if(len(enc_email) > 0):\n",
    "            n_clusters = int(np.ceil(len(enc_email)**0.5))\n",
    "            kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "            kmeans = kmeans.fit(enc_email)\n",
    "            avg = []\n",
    "            closest = []\n",
    "            for j in range(n_clusters):\n",
    "                idx = np.where(kmeans.labels_ == j)[0]\n",
    "                #print(\"IDX is: \", idx)\n",
    "                avg.append(np.mean(idx))\n",
    "            closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_,\\\n",
    "                                                       enc_email)\n",
    "            ordering = sorted(range(n_clusters), key=lambda k: avg[k])\n",
    "            summary[rv] = ' '.join([review[closest[idx]] for idx in ordering])\n",
    "            print(\"Done for review # = \", rv)\n",
    "        else:\n",
    "            print(\"This is not a valid review\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_5000 = df.iloc[:5000]\n",
    "df_5000['PredictedSummary'] = summary\n",
    "df_5000[['Text', 'PredictedSummary']].to_csv('top_5000_summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
